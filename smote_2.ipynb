{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Multiclass Classfifier 성능 비교 (with over-sampling methods)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","from numpy import mean\n","from numpy import std\n","import pandas as pd\n","import seaborn as sns\n","import imblearn\n","from collections import Counter\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import KFold\n","from imblearn.over_sampling import ADASYN \n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pandas import read_csv\n","\n","df = read_csv('glass.csv', header=None)\n","print(df.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[range(0,9)].hist()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 위의 분포로 보아 두가지 종류로  standardization을 한다\n","\n","mm=MinMaxScaler()\n","ss=StandardScaler()\n","df[[0,1,3,4,6]]=ss.fit_transform(df[[0,1,3,4,6]])\n","df[[2,5,7,8]] = mm.fit_transform(df[[2,5,7,8]])\n","\n","df[range(0,9)].hist()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#타겟의 불균형 정도를 파악\n","data=df.values\n","X, y = data[:, :-1], data[:, -1]\n","\n","# summarize distribution\n","counter = Counter(y)\n","for k,v in counter.items():\n","\tper = v / len(y) * 100\n","\tprint('Class=%d, n=%d (%.2f%%)' % (k, v, per))\n","# plot the distribution\n","plt.bar(counter.keys(), counter.values())\n","plt.show()\n","\n","\n","#향후 SMOTE와 Adasyn을 통해 오버샘플링을 진행할 것\n","#LabelEncoder를 통해 없는 타겟 변수는 지워줄 것이다."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#존재하지 않는 타겟 변수를 삭제하고 소수 클래스를 증강하면 다음과 같은 분포를 보일 것이다.\n","\n","from imblearn.over_sampling import SMOTE\n","\n","# transform the dataset\n","oversample = SMOTE()\n","X, y = oversample.fit_resample(X, y)\n","\n","y = LabelEncoder().fit_transform(y)\n","# summarize distribution\n","counter = Counter(y)\n","for k,v in counter.items():\n","\tper = v / len(y) * 100\n","\tprint('Class=%d, n=%d (%.2f%%)' % (k, v, per))\n","# plot the distribution\n","plt.bar(counter.keys(), counter.values())\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 데이터셋 준비. test_size는 20%만 hold-out 한다.\n","# 분리 후 각 데이터셋의 크기는 다음과 같다\n","\n","from sklearn.model_selection import train_test_split\n","\n","X, y = data[:, :-1], data[:, -1]\n","y = LabelEncoder().fit_transform(y)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 77)\n","print(len(X_train))\n","print(len(X_test))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# SVM의 SVC, KMeans, RandomForest 각 모델에 대해 타겟 불균형 상태로 accuracy를 파악한다.\n","#각 분류기는 분류 정확성을 높이기 데이터셋과 오버샘플링에 따른 대략적인 성능을 보고자 함이라서 \n","# sklearn의 default 값으로 세팅하였다.\n","\n","from sklearn.svm import SVC\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","\n"," \n","names=['SVM','KNN','RF']\n","models=[]\n","\n","X, y = data[:, :-1], data[:, -1]\n","y = LabelEncoder().fit_transform(y)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 77)\n"," \n","# define models to test\n","\n","svm = SVC(gamma='auto')\n","models.append(svm)\n","\t# KNN\n","KNN = KNeighborsClassifier()\n","models.append(KNN)\n","\n","# RF\n","RF=RandomForestClassifier(n_estimators=1000)\n","models.append(RF)\n","\n","results = list()\n","# evaluate each model\n","for i,model in enumerate(models):\n","\t# evaluate the model and store results\n","\tmodel.fit(X_train,y_train)\n","\tscores = cross_val_score(estimator=model, X=X_train, y=y_train, cv=10)\n","\tresults.append(scores)\n","\t# summarize performance\n","\tprint('>%s %.3f (%.3f)' % (names[i], mean(scores), std(scores)))\n","# plot the results\n","plt.boxplot(results, labels=names, showmeans=True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# test data에 대한 경우\n","\n","test_results = list()\n","\n","for i, model in enumerate(models):\n","    \t# evaluate the model and store results\n","\t#y_pred = model.predict(X_test)\n","\tacc=model.score(X_test,y_test)\n","\ttest_results.append(acc)\n","\t# summarize performance\n","\tprint('>%s %.3f (%.3f)' % (names[i],acc, acc*100 ))\n","# plot the results\n","plt.bar(names,test_results)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## 오버샘플링을 하지 않은 경우, 세 모델 모두 성능이 좋지 않은 것으로 보인다.\n","## 그중에서는 RF 모델이 상대적으로 우수하다."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## 세 모델에 대한 SMOTE oversampling을 시도한다.\n","## Training Set에 대하여\n","X, y = data[:, :-1], data[:, -1]\n","oversample = SMOTE(k_neighbors = 3)\n","y=LabelEncoder().fit_transform(y)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y,\n","                                                    test_size = 0.20, random_state = 77)\n","X_train, y_train = oversample.fit_resample(X_train, y_train)\n","\n","results = list()\n","\n","# evaluate each model\n","for i,model in enumerate(models):\n","\t# evaluate the model and store results\n","\tmodel.fit(X_train,y_train)\n","\tscores = cross_val_score(estimator=model, X=X_train, y=y_train, cv=10)\n","\tresults.append(scores)\n","\t# summarize performance\n","\tprint('>%s %.3f (%.3f)' % (names[i], mean(scores), std(scores)))\n","# plot the results\n","plt.boxplot(results, labels=names, showmeans=True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Test Set에 대한 경우\n","\n","test_results = list()\n","\n","for i, model in enumerate(models):\n","    \t# evaluate the model and store results\n","\t#y_pred = model.predict(X_test)\n","\tacc=model.score(X_test,y_test)\n","\ttest_results.append(acc)\n","\t# summarize performance\n","\tprint('>%s %.3f (%.3f)' % (names[i],acc, acc*100 ))\n","# plot the results\n","plt.bar(names,test_results)\n","plt.show()\n","\n","#smote에 의한 오버샘플링을 하지 않은 경우 보다 개선되었으나, test data와의 accuracy 차기 많이 존재한다."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## 세 모델에 대한 Adasyn oversampling을 시도한다.\n","## Training Set에 대하여\n","\n","from imblearn.over_sampling import ADASYN \n","\n","ada = ADASYN(sampling_strategy='minority',n_neighbors=3)\n","X, y = data[:, :-1], data[:, -1]\n","\n","y=LabelEncoder().fit_transform(y)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y,\n","                                                    test_size = 0.20, random_state = 77)\n","X_train, y_train = ada.fit_resample(X_train, y_train)\n","\n","results=[]\n","for i,model in enumerate(models):\n","    \t# evaluate the model and store results\n","\tmodel.fit(X_train,y_train)\n","\tscores = cross_val_score(estimator=model, X=X_train, y=y_train, cv=10)\n","\tresults.append(scores)\n","\t# summarize performance\n","\tprint('>%s %.3f (%.3f)' % (names[i], mean(scores), std(scores)))\n","# plot the results\n","plt.boxplot(results, labels=names, showmeans=True)\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# test data에 대한 경우\n","\n","test_results = list()\n","\n","for i, model in enumerate(models):\n","    # evaluate the model and store results\n","\t#y_pred = model.predict(X_test)\n","\tacc=model.score(X_test,y_test)\n","\ttest_results.append(acc)\n","\t# summarize performance\n","\tprint('>%s %.3f (%.3f)' % (names[i],acc, acc*100 ))\n","# plot the results\n","plt.bar(names,test_results)\n","plt.show()\n","\n","#smote에 비해 training accuracy는 낮지만, test set accuracy는 상대적으로 높다"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 종합: adasyn 오버샘플링을 적용한 RF의 성능이 가장 우수한 것으로 판단된다"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#우선 데이터셋의 크기가 매우 작으므로, 심층신경망은 잘 작동하지 않을 것으로 예상되나 일단 그 확인을 위해\n","# Training Set과 Test Set 을 나누지 않고 전체를 대상으로 베이스라인을 확인한다.\n","\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.wrappers.scikit_learn import KerasClassifier\n","from keras.utils import np_utils\n","from sklearn.pipeline import Pipeline\n","\n","# split into input and output elements\n","X, y = data[:, :-1], data[:, -1]\n","\n","ada = ADASYN(sampling_strategy='minority', n_neighbors=3)\n","X, y = ada.fit_resample(X, y)\n","y = LabelEncoder().fit_transform(y)\n","\n"," \n","# define baseline model\n","def baseline_model():\n","\t# create model\n","\tmodel = Sequential()\n","\tmodel.add(Dense(128, input_dim=9, activation='relu'))\n","\tmodel.add(Dense(64, input_dim=128, activation='relu'))\n","\tmodel.add(Dense(32, input_dim=64, activation='relu'))\n","\tmodel.add(Dense(6, activation='softmax'))\n","\t# Compile model\n","\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\treturn model\n","model=baseline_model()\n","estimator = KerasClassifier(build_fn=baseline_model, epochs=100, batch_size=16, verbose=0)\n","kfold = KFold(n_splits=10, shuffle=True)\n","results = cross_val_score(estimator, X, y, cv=kfold)\n","print(\"Baseline Model Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#위 시범 모델에 대한 accuracy의 분포는 다음과 같다\n","\n","label='NN'\n","plt.boxplot(results,labels=None,showmeans=True)\n","plt.xlabel('NN')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### 아래 세가지의 심층신경망들은 모두 L2정규화와 드롭아웃 0.5로 세팅하였고, \n","### 10 fold로 학습과정의 acc를 확인하였다."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#심층 신경망에 대해 오버샘플링을 하지 않은 모델\n","\n","from tensorflow import keras\n","from sklearn.model_selection import StratifiedKFold\n","from tensorflow.keras.utils import to_categorical\n","kfold = KFold(n_splits=10, shuffle=True, random_state=77)\n","cvscores = []\n","X, y = data[:, :-1], data[:, -1]\n","y = LabelEncoder().fit_transform(y)\n","y=to_categorical(y, num_classes=6, dtype='float32')\n","X_train, X_test, y_train, y_test = train_test_split(X, y,\n","                                                    test_size = 0.20, random_state = 77)\n","  # create model\n","acc_per_fold=[]\n","\n","for train, val in kfold.split(X_train, y_train):\n","\tmodel = Sequential()\n","\tmodel.add(Dense(128, input_dim=9, activation='relu',kernel_regularizer=keras.regularizers.l2(0.001)))\n","\tkeras.layers.Dropout(0.5),\n","\tmodel.add(Dense(64, input_dim=128, activation='relu',kernel_regularizer=keras.regularizers.l2(0.001)))\n","\tkeras.layers.Dropout(0.5),\n","\tmodel.add(Dense(32, input_dim=64, activation='relu',kernel_regularizer=keras.regularizers.l2(0.001)))\n","\tkeras.layers.Dropout(0.5),\n","\tmodel.add(Dense(6, activation='softmax'))\n","\t# Compile model\n","\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\t# Fit the model\n","\tmodel.fit(X_train[train], y_train[train], epochs=100, batch_size=16, verbose=0)\n","\t# evaluate the model\n","\tscores = model.evaluate(X_train[val], y_train[val], verbose=0)\n","\t\n","\tacc_per_fold.append(scores[1] * 100)\n","\t\n","print(\"Mean of training accuracy : {:.3f}%\".format(np.mean(acc_per_fold)))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#위 모델의 분포는 아래와 같다\n","\n","plt.boxplot(acc_per_fold,labels=None,showmeans=True)\n","plt.xlabel('Neural Net Model with original data')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_acc = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Test Accuracy : {:.3f}%\".format(test_acc[1]*100))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#뉴럴넷 with smote\n","\n","kfold = KFold(n_splits=5, shuffle=True, random_state=77)\n","cvscores = []\n","X, y = data[:, :-1], data[:, -1]\n","\n","oversample = SMOTE(k_neighbors=3)\n","\n","y = LabelEncoder().fit_transform(y)\n","y=to_categorical(y, num_classes=6, dtype='float32')\n","X_train, X_test, y_train, y_test = train_test_split(X, y,\n","                                                    test_size = 0.20, random_state = 77)\n","\n","X_train, y_train = oversample.fit_resample(X_train, y_train)\n","\n","acc_per_fold=[]\n","\n","for train, val in kfold.split(X_train, y_train):\n","\tmodel = Sequential()\n","\tmodel.add(Dense(128, input_dim=9, activation='relu',kernel_regularizer=keras.regularizers.l2(0.001)))\n","\tkeras.layers.Dropout(0.5),\n","\tmodel.add(Dense(64, input_dim=128, activation='relu',kernel_regularizer=keras.regularizers.l2(0.001)))\n","\tkeras.layers.Dropout(0.5),\n","\tmodel.add(Dense(32, input_dim=64, activation='relu',kernel_regularizer=keras.regularizers.l2(0.001)))\n","\tkeras.layers.Dropout(0.5),\n","\tmodel.add(Dense(6, activation='softmax'))\n","\t# Compile model\n","\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\t# Fit the model\n","\tmodel.fit(X_train[train], y_train[train], epochs=100, batch_size=16, verbose=0)\n","\t# evaluate the model\n","\tscores = model.evaluate(X_train[val], y_train[val], verbose=0)\n","\t\n","\tacc_per_fold.append(scores[1] * 100)\t\n"," \n","print(\"Mean of Train Accuracy : {:.3f}%\".format(np.mean(acc_per_fold)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.boxplot(acc_per_fold,labels=None,showmeans=True)\n","plt.xlabel('Neural Net with SMOTE')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_acc = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Test Accuracy : {:.3f}%\".format(test_acc[1]*100))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Neural Net with Adasyn\n","\n","kfold = KFold(n_splits=10, shuffle=True, random_state=77)\n","cvscores = []\n","X, y = data[:, :-1], data[:, -1]\n","\n","ada = ADASYN(sampling_strategy='minority',n_neighbors=3)\n","\n","y = LabelEncoder().fit_transform(y)\n","y=to_categorical(y, num_classes=6, dtype='float32')\n","X_train, X_test, y_train, y_test = train_test_split(X, y,\n","                                                    test_size = 0.20, random_state = 77)\n","X_train, y_train = ada.fit_resample(X_train, y_train)\n","\n","acc_per_fold=[]\n","\n","for train, val in kfold.split(X_train, y_train):\n","\tmodel = Sequential()\n","\tmodel.add(Dense(128, input_dim=9, activation='relu',kernel_regularizer=keras.regularizers.l2(0.001)))\n","\tkeras.layers.Dropout(0.5),\n","\tmodel.add(Dense(64, input_dim=128, activation='relu',kernel_regularizer=keras.regularizers.l2(0.001)))\n","\tkeras.layers.Dropout(0.5),\n","\tmodel.add(Dense(32, input_dim=64, activation='relu',kernel_regularizer=keras.regularizers.l2(0.001)))\n","\tkeras.layers.Dropout(0.5),\n","\tmodel.add(Dense(6, activation='softmax'))\n","\t# Compile model\n","\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\t# Fit the model\n","\tmodel.fit(X_train[train], y_train[train], epochs=100, batch_size=16, verbose=0)\n","\t# evaluate the model\n","\tscores = model.evaluate(X_train[val], y_train[val], verbose=0)\n","\t\n","\tacc_per_fold.append(scores[1] * 100)\n","\t\n","print(\"Mean of Train Accuracy : {:.3f}%\".format(np.mean(acc_per_fold)))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.boxplot(acc_per_fold,labels=None,showmeans=True)\n","plt.xlabel('Neural Net with Adasyn')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_acc = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Test Accuracy : {:.3f}%\".format(test_acc[1]*100))"]},{"cell_type":"markdown","metadata":{},"source":["### 결론 \n","\n","뉴럴넷에 adasyn을 적용한 모델이 가장 test acc가 높았다.\n","그러나 고전적 ML 모델은 물론 NN 모델 모두 하이퍼 파라미터 fine-tuning을 하지 않은 상태이므로, \n","단순 비교는 지양할 필요가 있다.\n","\n","다만, SVM은 본 보고의 데이터 셋과 같은 작은 데이터셋에 잘 작용하지 않는 것으로 보였다.\n","데이터셋과 관련해서는, \"하지 않은 경우\" < Smote < Adasyn 의 순서로 보이나, 각 모델별로 차이가 있다.\n"]},{"cell_type":"markdown","metadata":{},"source":["## 중요\n","\n","#### 오버샘플링을 할 경우, 반드시 train data와 test data를 나눈 후, train data에 대해서만 해야 함\n","#### 그렇지 않고 만약 test data에도 오버샘플링을 할 경우, 비정상적으로 test accuracy가 높게 나옴\n","#### test data에도 오버샘플링을 할 경우 training 과정에서 이미 목격된 데이터가 test set에도 존재할 가능성이 커지기 때문임"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"smote.ipynb","provenance":[]},"interpreter":{"hash":"dc4cb51046b5730eccdebbac3fcc03a5de78bd3fb599443e44d1810e5c7dd8cd"},"kernelspec":{"display_name":"Python 3.8.8 64-bit ('base': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
